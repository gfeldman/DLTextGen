{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings to predict sentiment of movie reviews with the IMDB dataset (Basic Model)\n",
    "\n",
    "\n",
    "From: Deep Learning with Python (Starting at Listing 6.6)\n",
    "\n",
    "Reproduced by: Guy Feldman\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# cut texts after this number of words\n",
    "max_len =  500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IMDB Dataset\n",
    "\n",
    "- The features are vectors of word indices that represent a review\n",
    "- The output variable, y, indicates whether a review was positive or negative.\n",
    "\n",
    "The argument num_words=10000 means that we will only keep the top 10,000 most frequently occurring words in the training set. Rare words will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we get indices of words and not the actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "        113,  103,   32,   15,   16, 5345,   19,  178,   32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_pad = np.array(x_train[0][-20:])\n",
    "before_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i-3,'#?#') for i in before_pad])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pad_sequences function takes the last max_len word indices from a review. If the review doesn't have max_len word indices, zeros are appended to the begining of the sequence so that all sequences have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,   14,   22,   16,\n",
       "         43,  530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,\n",
       "        173,   36,  256,    5,   25,  100,   43,  838,  112,   50,  670,\n",
       "          2,    9,   35,  480,  284,    5,  150,    4,  172,  112,  167,\n",
       "          2,  336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,\n",
       "         13,  447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,\n",
       "         22,    4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,\n",
       "         43,  530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,\n",
       "         17,   12,   16,  626,   18,    2,    5,   62,  386,   12,    8,\n",
       "        316,    8,  106,    5,    4, 2223, 5244,   16,  480,   66, 3785,\n",
       "         33,    4,  130,   12,   16,   38,  619,    5,   25,  124,   51,\n",
       "         36,  135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,\n",
       "         77,   52,    5,   14,  407,   16,   82,    2,    8,    4,  107,\n",
       "        117, 5952,   15,  256,    4,    2,    7, 3766,    5,  723,   36,\n",
       "         71,   43,  530,  476,   26,  400,  317,   46,    7,    4,    2,\n",
       "       1029,   13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,\n",
       "         56,   26,  141,    6,  194, 7486,   18,    4,  226,   22,   21,\n",
       "        134,  476,   26,  480,    5,  144,   30, 5535,   18,   51,   36,\n",
       "         28,  224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,\n",
       "         88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,\n",
       "         16, 5345,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_pad= x_train[0]\n",
    "after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "        113,  103,   32,   15,   16, 5345,   19,  178,   32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model (regression on the words)\n",
    "\n",
    "To build this model, we:\n",
    "1. add an embedding layer that will take the input vectors representing sentences (that live in $\\mathbb{N}^{\\text{max_len}}$) and embed each of the words in $\\mathbb{R}^{32}$; \n",
    "2. flatten the tensor so that each row corresponds to a \"sentence\" with each word being represented by its embedded value.\n",
    "3. Feed the sentence into a sigmoid layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16001     \n",
      "=================================================================\n",
      "Total params: 336,001\n",
      "Trainable params: 336,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output_dim = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,output_dim,input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.4813 - acc: 0.7717 - val_loss: 0.3071 - val_acc: 0.8770\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.2305 - acc: 0.9112 - val_loss: 0.2829 - val_acc: 0.8818\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.1604 - acc: 0.9407 - val_loss: 0.2736 - val_acc: 0.8882\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.1104 - acc: 0.9628 - val_loss: 0.2915 - val_acc: 0.8856\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0725 - acc: 0.9774 - val_loss: 0.3132 - val_acc: 0.8824\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0448 - acc: 0.9879 - val_loss: 0.3454 - val_acc: 0.8790\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0261 - acc: 0.9937 - val_loss: 0.3916 - val_acc: 0.8708\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0144 - acc: 0.9966 - val_loss: 0.4315 - val_acc: 0.8724\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0075 - acc: 0.9986 - val_loss: 0.4725 - val_acc: 0.8718\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.5365 - val_acc: 0.8660\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "1. We obtain an approximate accuracy of 0.87, which is not bad.\n",
    "\n",
    "## Model Limitation\n",
    "Since we simply flatten the words and feed them into a classifier, we are not taking into account the order of the words. (e.g. it would likely treat both \"this movie is shit\" and \"this movie is the shit\" as being negative \"reviews\"). It would be much better if we could add a recurrent layer to capture sequences of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
