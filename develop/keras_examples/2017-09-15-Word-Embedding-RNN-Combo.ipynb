{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings to predict sentiment of movie reviews with the IMDB dataset (Basic model with an RNN layer)\n",
    "\n",
    "\n",
    "From: Deep Learning with Python (Starting at Listing 6.27)\n",
    "\n",
    "Reproduced by: Guy Feldman\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# cut texts after this number of words\n",
    "max_len =  500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IMDB Dataset\n",
    "\n",
    "- The features are vectors of word indices that represent a review\n",
    "- The output variable, y, indicates whether a review was positive or negative.\n",
    "\n",
    "The argument num_words=10000 means that we will only keep the top 10,000 most frequently occurring words in the training set. Rare words will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model\n",
    "## regression on the words\n",
    "\n",
    "To build this model w\n",
    "1. add an embedding layer that will take the input vectors representing sentences (that live in $\\mathbb{N}^{\\text{max_len}}$) and embed each of the words in $\\mathbb{R}^{8}$; \n",
    "2. flatten the tensor so that each row corresponds to a \"sentence\" with each word being represented by its embedded value.\n",
    "3. Feed the sentence into a sigmoid layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output_dim = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,output_dim,input_length=max_len))\n",
    "model.add(SimpleRNN(output_dim))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 55s - loss: 0.5638 - acc: 0.6907 - val_loss: 0.4045 - val_acc: 0.8244\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 54s - loss: 0.3525 - acc: 0.8525 - val_loss: 0.3769 - val_acc: 0.8354\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 54s - loss: 0.2943 - acc: 0.8835 - val_loss: 0.3994 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 54s - loss: 0.2471 - acc: 0.9038 - val_loss: 0.3609 - val_acc: 0.8596\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 54s - loss: 0.2030 - acc: 0.9213 - val_loss: 0.4360 - val_acc: 0.8186\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 54s - loss: 0.1644 - acc: 0.9390 - val_loss: 0.5359 - val_acc: 0.7798\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 52s - loss: 0.1219 - acc: 0.9569 - val_loss: 0.6297 - val_acc: 0.7686\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 53s - loss: 0.1037 - acc: 0.9641 - val_loss: 0.5198 - val_acc: 0.8278\n",
      "Epoch 9/10\n",
      "14592/20000 [====================>.........] - ETA: 13s - loss: 0.0711 - acc: 0.9752"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Limitation\n",
    "\n",
    "Since we simply flatten the words and feed them into a classifier, we are not taking into account the order of the words. (e.g. it would likely treat both \"this movie is shit\" and \"this movie is the shit\" as being negative \"reviews\"). It would be much better if we could add a recurrent layer to capture sequences of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
